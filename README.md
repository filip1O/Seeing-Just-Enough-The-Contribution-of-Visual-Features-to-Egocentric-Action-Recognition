# Seeing Just Enough: The Minimal Visual Ingredients of Egocentric Action Recognition

This repository accompanies the study **â€œSeeing Just Enough: The Minimal Visual Ingredients of Egocentric Action Recognitionâ€**, investigating the spatial and temporal features that make everyday actions recognisable from first-person (egocentric) video.

---

## ğŸ“˜ Overview

Humans can recognise complex actions from very limited visual information â€” sometimes just a few frames or small image regions.
In this project, we identify **the minimal visual features** required for recognising actions in naturalistic egocentric videos from the [EPIC-Kitchens-100 dataset](https://epic-kitchens.github.io/).

Two main experiments were conducted:

1. **Experiment 1 â€“ Human Ground Truth (HGT):**  
   Participants labelled short egocentric videos of everyday actions (e.g., *opening fridge*, *putting down cup*).  
   A custom **language-model framework (SBERT + Word2Vec)** quantified the semantic consistency of responses to derive robust *Human Ground Truth* labels.

2. **Experiment 2 â€“ Iterative Reduction:**  
   Consistently recognised videos were recursively **cropped** and **temporally scrambled** to identify the *Minimal Recognisable Configurations* (MIRCs) and the spatiotemporal features whose removal causes recognition breakdown.  
   Recognition was modelled using **linear mixed-effects models** and **random-forest feature classifiers** (with SHAP and Boruta analysis) on both high-level (hands, objects) and low/mid-level (orientation, motion, colour, etc.) visual features.

---

## ğŸ§  Key Findings

- Recognition breaks down abruptly once critical **handâ€“object and orientation features** are removed.  
- *Easy* and *Hard* videos (based on network action classifier accuracy) differ not only in recognisability by humans but in the **relative weight of spatial and spatiotemporal features**.  
- MIRCs typically occupied **~3â€“5 %** of the original visual scene, yet preserved enough information for accurate recognition.

---

### SBERT Framework Used for Computing Semantic Similarity Between Responses

![SBERT framework](images/Figure_1.png)

---

### Spatial and Temporal Reduction Process

![Reduction Flow](images/Figure_4.png)

---

## ğŸ“‚ Repository Structure

```text
ğŸ“ Human Ground Truth/
â”œâ”€ ğŸ“ demos/
â”œâ”€ "SBERT_framework_response-response.py" - script used to quantify pair-wise Semantic Similarities between responses using SBERT
â”œ
â”œâ”€ ğŸ“ example_video_responses/
â”œâ”€ "10788.csv" - example file with responses to video 10788, the "SBERT_framework_response-response.py" script can be applied to this file to construct the semantic space of responses
â”œ
â””â”€ "HGT_master.xlsx" - Human Ground Truth, Recognition Consistency and other descriptive statistics for each video


ğŸ“ Reduction Experiment/
â”œâ”€ ğŸ“ binary_classification_sets_and_results/ - sets used for classification + classification results of each comparison
â”œ
â”œâ”€ ğŸ“ demos/
â”œâ”€ "loo_randomforest_classification_and_shap.py" - base script used for classification and SHAP (can be ran on provided classification sets)
â”œâ”€ "loo_randomforest_boruta_bootstrap.py" - script used to produce the Boruta threshold and assess the significance of SHAP feature importance (can be ran on provided classification sets)
â”œâ”€ "SBERT_framework_response-GT.py" - script used to compute Semantic Similarities between responses and Ground Truths
â”œ
â”œâ”€ ğŸ“ example_video_responses/
â”œâ”€ "pooled_LL_LL_LL_03159.csv" - example file with responses to quadrant LL_LL_LL_03159, the "SBERT_framework_response-GT.py" script can be applied to this file to compute semantic similarities between HGT and responses
â”œ
â”œâ”€ ğŸ“ segmentation_masks/ - .mp4 files with segmentations, colour coding spreadhseet and reasoning for segmented objects
â”œ
â””â”€ "main_reduction_master.xlsx" - results of participant testing in Experiment 2 for each quadrant and various descriptive statistics

```
---

## ğŸ“¦ Human Ground Truth Videos
The full database of videos used in Experiment 1 of this project is available at:
[Download from OSF](https://osf.io/cxnm8)

---

## ğŸ“¦ Epic-ReduAct
The full database of videos, cropped quadrants, and MIRCs used in Experiment 2 of this project is available in the companion repository:  
[**SadeghRahmaniB/Epic-ReduAct**](https://github.com/SadeghRahmaniB/Epic-ReduAct)

These files can be directly mapped to corresponding analyses and metadata in this repository.

---

## ğŸ” Dependencies

- **Python 3.11+**  
  `pandas`, `numpy`, `sentence-transformers`, `gensim`, `scikit-learn`, `shap`, `matplotlib`, `seaborn`, `openpyxl`, `joblib`, `spacy`, `scipy`

---

## âš™ï¸ Running the Analysis Demos

1. Clone this repository  
   ```bash
   git clone https://github.com/filip1O/seeing-just-enough.git
   cd seeing-just-enough
   
2. Create a Python environment and install requirements:
    ```bash
    pip install -r requirements.txt

3. Run feature extraction and classification pipelines

---

## ğŸ“Š Citation

If you use these materials or build on this work, please cite:

Rybansky, F. (2025). Seeing Just Enough: The Minimal Visual Ingredients of Egocentric Action Recognition.

---

## ğŸ“œ License

All code is released under the MIT License.
Stimuli and derived data (cropped EPIC-Kitchens-100 videos) are shared under Creative Commons BY-NC 4.0, in accordance with the original dataset license.

---

## ğŸ§© Contact

For questions or collaboration:
Filip Rybansky
Newcastle University, Biosciences Institute
ğŸ“§ f.rybansky2@newcastle.ac.uk
